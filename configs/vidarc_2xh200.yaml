# Stage 2: Vidarc causal training - optimized for 2x H200 (120GB each)
# Model: WanModelCausal
# Loss: Causal flow matching + embodiment-aware

model:
  ckpt_dir: checkpoints/Wan2.2-TI2V-5B
  pt_dir: checkpoints/vidar/vidar.pt
  model_class: WanModelCausal
  gradient_checkpointing: true

training:
  num_steps: 4000
  batch_size: 16                    # 8 per GPU
  gradient_accumulation: 8          # Effective batch = 128
  lr: 2.0e-5
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.0e-8
  warmup_steps: 200
  scheduler: cosine
  freeze:
    - t5
    - vae

data:
  data_dir: data/robotwin
  num_frames: 81
  resolution: [736, 640]
  fps: 10
  num_workers: 4
  pin_memory: true
  cfg_prob: 0.1

loss:
  type: causal_flow_matching
  embodiment_aware: true
  eta: 3.0

self_forcing:
  enabled: true
  causal: true
  chunk_size: 8                     # Reduced from 16 for memory
  kv_cache_length: 64
  same_step_across_blocks: true

distributed:
  use_fsdp: true
  sharding_strategy: FULL_SHARD     # Max memory efficiency
  mixed_precision: bf16
  activation_checkpointing: true
  cpu_offload: false                # Enable if OOM

logging:
  output_dir: outputs/vidarc_2xh200
  log_interval: 50
  save_interval: 500
  eval_interval: 1000
  use_wandb: true
  wandb_project: vidarc-training

output:
  save_path: checkpoints/vidar/vidarc.pt
  save_optimizer: true
  save_scheduler: true

seed: 42
