W0118 15:45:12.668000 641 site-packages/torch/distributed/run.py:793] 
W0118 15:45:12.668000 641 site-packages/torch/distributed/run.py:793] *****************************************
W0118 15:45:12.668000 641 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0118 15:45:12.668000 641 site-packages/torch/distributed/run.py:793] *****************************************
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar/wan/modules/model_causal.py:18: UserWarning: using FA2
  warnings.warn('using FA2')
2026-01-18 15:45:17,301 - root - INFO - torch._dynamo cache_size_limit set to 64
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar/wan/modules/model_causal.py:18: UserWarning: using FA2
  warnings.warn('using FA2')
2026-01-18 15:45:17,305 - root - INFO - torch._dynamo cache_size_limit set to 64
2026-01-18 15:45:17,307 - __main__ - INFO - ============================================================
2026-01-18 15:45:17,308 - __main__ - INFO - Vidarc Stage 2 Training: Causal Self-Forcing
2026-01-18 15:45:17,308 - __main__ - INFO - ============================================================
2026-01-18 15:45:17,308 - __main__ - INFO - World size: 2
2026-01-18 15:45:17,308 - __main__ - INFO - Device: cuda:0
2026-01-18 15:45:17,314 - __main__ - INFO - Saved config to output_vidarc/config.yaml
2026-01-18 15:45:17,314 - __main__ - INFO - Creating Vidarc causal trainer...
2026-01-18 15:45:17,420 - training.trainers.vidarc_trainer - INFO - Setting up Vidarc causal trainer (Stage 2)...
2026-01-18 15:45:17,420 - training.trainers.vidarc_trainer - INFO - Building causal model...
2026-01-18 15:45:17,420 - training.trainers.vidarc_trainer - INFO - Building WanModelCausal from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B
2026-01-18 15:45:17,420 - training.models.wrapper_causal - INFO - Loading T5 encoder...
2026-01-18 15:45:17,420 - training.models.wrapper_causal - INFO - Rank 1 loading T5 (batch 1)...
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar-robotwin/wandb/offline-run-20260118_154518-6zq2q5dk
2026-01-18 15:45:18,618 - training.trainers.vidarc_trainer - INFO - Setting up Vidarc causal trainer (Stage 2)...
2026-01-18 15:45:18,618 - training.trainers.vidarc_trainer - INFO - Building causal model...
2026-01-18 15:45:18,618 - training.trainers.vidarc_trainer - INFO - Building WanModelCausal from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B
2026-01-18 15:45:18,618 - training.models.wrapper_causal - INFO - Loading T5 encoder...
2026-01-18 15:45:18,618 - training.models.wrapper_causal - INFO - Rank 0 loading T5 (batch 1)...
2026-01-18 15:45:48,540 - root - INFO - loading /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B/models_t5_umt5-xxl-enc-bf16.pth
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar/wan/modules/t5.py:500: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(checkpoint_path, map_location=device))
2026-01-18 15:45:49,175 - root - INFO - loading /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B/models_t5_umt5-xxl-enc-bf16.pth
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar/wan/modules/t5.py:500: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(checkpoint_path, map_location=device))
2026-01-18 15:46:14,300 - training.models.wrapper_causal - INFO - Rank 0 finished loading T5
2026-01-18 15:46:14,308 - training.models.wrapper_causal - INFO - Rank 1 finished loading T5
[rank1]:[W118 15:46:14.936954210 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W118 15:46:14.991009650 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
2026-01-18 15:46:14,948 - training.models.wrapper_causal - INFO - Loading VAE...
2026-01-18 15:46:14,948 - training.models.wrapper_causal - INFO - Loading VAE...
2026-01-18 15:46:14,948 - training.models.wrapper_causal - INFO - Rank 1 loading VAE (batch 1)...
2026-01-18 15:46:14,949 - training.models.wrapper_causal - INFO - Rank 0 loading VAE (batch 1)...
2026-01-18 15:46:14,964 - root - INFO - loading /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B/Wan2.2_VAE.pth
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar/wan/modules/vae2_2.py:889: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(pretrained_path, map_location=device), assign=True)
2026-01-18 15:46:14,964 - root - INFO - loading /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B/Wan2.2_VAE.pth
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar/wan/modules/vae2_2.py:889: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(pretrained_path, map_location=device), assign=True)
2026-01-18 15:46:21,035 - training.models.wrapper_causal - INFO - Rank 1 finished loading VAE
2026-01-18 15:46:21,035 - training.models.wrapper_causal - INFO - Rank 0 finished loading VAE
2026-01-18 15:46:21,036 - training.models.wrapper_causal - INFO - Loading WanModelCausal from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B
2026-01-18 15:46:21,036 - training.models.wrapper_causal - INFO - Loading WanModelCausal from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B
2026-01-18 15:46:21,036 - training.models.wrapper_causal - INFO - Rank 1 loading DiT (batch 1)...
2026-01-18 15:46:21,036 - training.models.wrapper_causal - INFO - Rank 0 loading DiT (batch 1)...
2026-01-18 15:46:39,018 - training.models.wrapper_causal - INFO - Loading Stage 1 weights from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/vidar_ckpts/vidar.pt
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar-robotwin/training/models/wrapper_causal.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(self.pt_dir, map_location="cpu")
2026-01-18 15:46:39,030 - training.models.wrapper_causal - INFO - Loading Stage 1 weights from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/vidar_ckpts/vidar.pt
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar-robotwin/training/models/wrapper_causal.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(self.pt_dir, map_location="cpu")
2026-01-18 15:48:00,535 - training.models.wrapper_causal - INFO - Rank 1 finished loading DiT
2026-01-18 15:48:00,535 - training.models.wrapper_causal - INFO - Rank 0 finished loading DiT
2026-01-18 15:48:00,536 - training.models.wrapper_causal - INFO - Freezing T5 encoder
2026-01-18 15:48:00,536 - training.models.wrapper_causal - INFO - Freezing T5 encoder
2026-01-18 15:48:00,538 - training.models.wrapper_causal - INFO - Freezing VAE
2026-01-18 15:48:00,538 - training.models.wrapper_causal - INFO - Freezing VAE
2026-01-18 15:48:00,539 - training.trainers.vidarc_trainer - INFO - Wrapping DiT with FSDP...
2026-01-18 15:48:00,539 - training.trainers.vidarc_trainer - INFO - Wrapping DiT with FSDP...
2026-01-18 15:48:07,831 - training.trainers.vidarc_trainer - INFO - Building optimizer and scheduler...
2026-01-18 15:48:07,833 - training.trainers.vidarc_trainer - INFO - Trainable parameters: 2,499,893,856
2026-01-18 15:48:07,834 - training.trainers.vidarc_trainer - INFO - Building dataloader...
2026-01-18 15:48:07,834 - training.trainers.base - INFO - Using HDF5 dataset from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/datasets/RoboTwin2.0/dataset/processed
2026-01-18 15:48:07,835 - training.data.hdf5_dataset - INFO - Searching for episodes in: /mnt/shared-storage-user/qinyiran/cyujie/cyujie/datasets/RoboTwin2.0/dataset/processed/hdf5
2026-01-18 15:48:07,836 - training.data.hdf5_dataset - INFO - Found 50 files matching 'episode_*.hdf5'
2026-01-18 15:48:07,836 - training.data.hdf5_dataset - INFO - Found 50 episodes in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/datasets/RoboTwin2.0/dataset/processed
2026-01-18 15:48:07,837 - training.trainers.vidarc_trainer - INFO - Moving VAE to GPU for faster encoding
2026-01-18 15:48:08,256 - training.trainers.vidarc_trainer - INFO - Setup complete! Chunk size: 1
2026-01-18 15:48:08,257 - __main__ - INFO - ============================================================
2026-01-18 15:48:08,257 - __main__ - INFO - Stage 2 Training Configuration:
2026-01-18 15:48:08,257 - __main__ - INFO -   - Data dir: /mnt/shared-storage-user/qinyiran/cyujie/cyujie/datasets/RoboTwin2.0/dataset/processed
2026-01-18 15:48:08,257 - __main__ - INFO -   - Checkpoint dir: /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/Wan2.2-TI2V-5B
2026-01-18 15:48:08,257 - __main__ - INFO -   - Stage 1 weights: /mnt/shared-storage-user/qinyiran/cyujie/cyujie/mounts/qinyiran/vidar/vidar_ckpts/vidar.pt
2026-01-18 15:48:08,257 - __main__ - INFO -   - Output dir: ./output_vidarc
2026-01-18 15:48:08,257 - __main__ - INFO -   - Max steps: 4000
2026-01-18 15:48:08,257 - __main__ - INFO -   - Batch size: 1 per GPU
2026-01-18 15:48:08,257 - __main__ - INFO -   - Learning rate: 2e-05
2026-01-18 15:48:08,257 - __main__ - INFO -   - Chunk size: 1
2026-01-18 15:48:08,257 - __main__ - INFO -   - Embodiment loss: False (eta=3.0)
2026-01-18 15:48:08,257 - __main__ - INFO - ============================================================
2026-01-18 15:48:08,258 - training.trainers.base - INFO - Starting training for 4000 steps...
2026-01-18 15:48:08,434 - training.trainers.vidarc_trainer - INFO - Building optimizer and scheduler...
2026-01-18 15:48:08,435 - training.trainers.vidarc_trainer - INFO - Trainable parameters: 2,499,893,856
2026-01-18 15:48:08,436 - training.trainers.vidarc_trainer - INFO - Building dataloader...
2026-01-18 15:48:08,436 - training.trainers.base - INFO - Using HDF5 dataset from /mnt/shared-storage-user/qinyiran/cyujie/cyujie/datasets/RoboTwin2.0/dataset/processed
2026-01-18 15:48:08,436 - training.data.hdf5_dataset - INFO - Searching for episodes in: /mnt/shared-storage-user/qinyiran/cyujie/cyujie/datasets/RoboTwin2.0/dataset/processed/hdf5
2026-01-18 15:48:08,437 - training.data.hdf5_dataset - INFO - Found 50 files matching 'episode_*.hdf5'
2026-01-18 15:48:08,437 - training.data.hdf5_dataset - INFO - Found 50 episodes in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/datasets/RoboTwin2.0/dataset/processed
2026-01-18 15:48:08,439 - training.trainers.vidarc_trainer - INFO - Moving VAE to GPU for faster encoding
2026-01-18 15:48:08,868 - training.trainers.vidarc_trainer - INFO - Setup complete! Chunk size: 1
2026-01-18 15:48:08,869 - training.trainers.base - INFO - Starting training for 4000 steps...
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar-robotwin/training/trainers/base.py:344: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.config.distributed.mixed_precision != "fp32"):
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar-robotwin/training/trainers/base.py:344: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.config.distributed.mixed_precision != "fp32"):
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460]
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar-robotwin/training/trainers/base.py:344: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.config.distributed.mixed_precision != "fp32"):
/mnt/shared-storage-user/qinyiran/cyujie/cyujie/code/vidar-robotwin/training/trainers/base.py:344: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.config.distributed.mixed_precision != "fp32"):
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
2026-01-18 15:50:54,574 - training.trainers.base - INFO - Step 0 | loss: 2.2942 | lr: 0.0000
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
2026-01-18 15:51:50,967 - training.trainers.vidarc_trainer - INFO - Saved Stage 2 checkpoint to checkpoints/vidar/vidarc.pt
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:1
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
DEBUG get_flex_causal_block_mask_for_prefill: num_denoise_block=1, block_size=460, seq_len=460, device=cuda:0
DEBUG get_flex_causal_block_mask_for_prefill: Q_LEN=460, KV_LEN=460
DEBUG get_flex_causal_block_mask_for_prefill: cache_size_limit=64, unique_KV_LEN_count=1, KV_LEN_history=[460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
DEBUG get_flex_causal_block_mask_for_prefill: Created block_mask successfully
[rank0]:[E118 16:05:23.739761196 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1454, OpType=_ALLGATHER_BASE, NumelIn=81828352, NumelOut=163656704, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
[rank0]:[E118 16:05:23.745359748 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 1454, last enqueued NCCL work: 1457, last completed NCCL work: 1453.
[rank0]:[E118 16:05:25.856869092 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 1454, last enqueued NCCL work: 1457, last completed NCCL work: 1453.
[rank0]:[E118 16:05:25.856897083 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E118 16:05:25.856903117 ProcessGroupNCCL.cpp:636] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E118 16:05:25.896885992 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1454, OpType=_ALLGATHER_BASE, NumelIn=81828352, NumelOut=163656704, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd118d6c446 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fd0cec2a762 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fd0cec31ba3 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fd0cec3360d in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fd11927a5c0 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fd11e12eac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fd11e1c0850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1454, OpType=_ALLGATHER_BASE, NumelIn=81828352, NumelOut=163656704, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd118d6c446 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fd0cec2a762 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fd0cec31ba3 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fd0cec3360d in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fd11927a5c0 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fd11e12eac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fd11e1c0850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fd118d6c446 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe42745 (0x7fd0ce8a0745 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fd11927a5c0 in /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7fd11e12eac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x7fd11e1c0850 in /lib/x86_64-linux-gnu/libc.so.6)

W0118 16:05:28.177000 641 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 660 closing signal SIGTERM
E0118 16:05:34.231000 641 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 659) of binary: /mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/bin/python3.10
Traceback (most recent call last):
  File "/mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/shared-storage-user/qinyiran/cyujie/cyujie/env/self_forcing/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/train_vidarc.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-18_16:05:28
  host      : gen3d-s2jwt-901431-worker-0.qinyiran.ailab-ma4agismall.svc.pjlab.local
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 659)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 659
============================================================
